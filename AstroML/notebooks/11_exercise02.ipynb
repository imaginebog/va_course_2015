{
 "metadata": {
  "name": "11_exercise02"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise 2: Photometric Redshift Determination"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we will take a closer look at the photometric redshift problem discussed\n",
      "in section 5 of the tutorial.  Using the decision tree classifier, we'll take\n",
      "a look at the 4-color observations of just over 400,000 galaxies observed by\n",
      "the Sloan Digital Sky Survey.\n",
      "\n",
      "We're trying to determine photometric redshifts of galaxies, which are related\n",
      "to their distances, and useful in a wide variety of cosmological studies.\n",
      "The point of this exercise is to answer the question: how can we get the rms\n",
      "error down to below 0.1?  Would it be a better use of telescope time to\n",
      "**observe more objects**, or to **observe additional features** of the objects\n",
      "in the data set?  To answer this, we'll make use of the learning curve\n",
      "techniques discussed in section 3 of the tutorial.\n",
      "\n",
      "Because we're going to be plotting things below, we'll first make sure we're in\n",
      "ipython's pylab mode:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as pl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial assumes the notebook is within the tutorial directory\n",
      "structure, and that the ``fetch_data.py`` script has been run\n",
      "to download the data locally.\n",
      "If the data is in a different location, you can change the\n",
      "``DATA_HOME`` variable below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "DATA_HOME = os.path.abspath('../data/sdss_photoz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "data = np.load(os.path.join(DATA_HOME, 'sdss_photoz.npy'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we'll truncate the data to 50,000 points.  This will allow the code\n",
      "below to be run quickly while it's being written.  When we're satisfied\n",
      "that the code is ready to go, we can comment out this line and re-run\n",
      "the analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = data[:50000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now as usual, we'll put the data into the matrix form expected\n",
      "by scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '%i points' % data.shape[0]\n",
      "u, g, r, i, z = [data[f] for f in 'ugriz']\n",
      "\n",
      "X = np.zeros((len(data), 4))\n",
      "X[:, 0] = u - g\n",
      "X[:, 1] = g - r\n",
      "X[:, 2] = r - i\n",
      "X[:, 3] = i - z\n",
      "\n",
      "y = data['redshift']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's divide our data into the training, cross-validation,\n",
      "and test samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Ntot = len(y)\n",
      "\n",
      "Ncv = Ntot / 5\n",
      "Ntest = Ntot / 5\n",
      "Ntrain = Ntot - Ncv - Ntest\n",
      "\n",
      "X_train = X[:Ntrain]\n",
      "y_train = y[:Ntrain]\n",
      "\n",
      "X_cv = X[Ntrain:Ntrain + Ncv]\n",
      "y_cv = y[Ntrain:Ntrain + Ncv]\n",
      "\n",
      "X_test = X[Ntrain + Ncv:]\n",
      "y_test = y[Ntrain + Ncv:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A Basic Decision Tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall from the tutorial the use of the decision tree for regression:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "clf = DecisionTreeRegressor(max_depth=15)\n",
      "clf.fit(X_train, y_train)\n",
      "y_pred = clf.predict(X_cv)\n",
      "\n",
      "pl.scatter(y_cv, y_pred, c='black', s=4, lw=0)\n",
      "pl.plot([0, 3], [0, 3], '-', color='gray')\n",
      "\n",
      "pl.xlabel('true redshift')\n",
      "pl.ylabel('predicted redshift')\n",
      "\n",
      "pl.xlim(0, 2.5)\n",
      "pl.ylim(0, 2.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this simple model, the points are generally clustered toward the\n",
      "true values, but there are many *catastrophic outliers*: i.e. points which\n",
      "have predicted redshifts very far from their true redshifts."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 1: Improving our Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the first part of this exercise, we'll use the cross-validation\n",
      "techniques discussed earlier to determine the best hyper-parameter\n",
      "choice for the decision tree regressor. Here you'll plot\n",
      "the training error and cross-validation error as a function of the\n",
      "meta-parameter ``max_depth``.\n",
      "\n",
      "We'll first define a convenience function to compute the rms error:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def compute_rms_error(y_pred, y_true):\n",
      "    return np.sqrt(metrics.mean_squared_error(y_pred, y_true))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the first part of this exercise, you will create three arrays:\n",
      "``max_depth_array``, ``train_error``, and ``cv_error``.\n",
      "Use at least 10 different values of ``max_depth``, and compute the training\n",
      "and cross-validation error associated with each of them.\n",
      "for power-users, note that this is a natural application for\n",
      "ipython's parallel processing capabilities, but that's beyond\n",
      "the scope of this tutorial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "# we'll explore results for max_depth from 1 to 20\n",
      "max_depth_array = np.arange(1, 21)\n",
      "train_error = np.zeros(len(max_depth_array))\n",
      "cv_error = np.zeros(len(max_depth_array))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here you'll write the code to compute the training error and\n",
      "# test error for the Decision Tree Regressor at each value of\n",
      "# max_depth.  You can type DecisionTreeRegressor? to see its\n",
      "# documentation."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the notebook is within the tutorial directory structure,\n",
      "the following command will load the solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load soln/02-01.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll plot the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.plot(max_depth_array, cv_error, label='cross-val error')\n",
      "pl.plot(max_depth_array, train_error, label='training error')\n",
      "\n",
      "pl.legend(loc=0)\n",
      "pl.xlabel('max depth')\n",
      "pl.ylabel('error')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should see a minimum of the cross-validation error for\n",
      "``max_depth`` around 8.  We can automatically find the optimal\n",
      "value using ``numpy.argmin``:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# select the value of max_depth which led to the best results\n",
      "max_depth = max_depth_array[np.argmin(cv_error)]\n",
      "print \"max_depth =\", max_depth\n",
      "print \"cross-val error = \", cv_error.min()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we see that after using cross-validation to avoid over-fitting,\n",
      "a simple decision tree got our cross-validation error down to around 0.2."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 2: Improving our Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we have a simple decision tree estimator which gives\n",
      "a cross-validation error around 0.22.  How can we improve on this?\n",
      "We could certainly use a more sophisticated model: that will be\n",
      "explored below.  But how could our data be made better?\n",
      "\n",
      "As noted during the tutorial, there are several options:\n",
      "\n",
      "- get more training data.  This involves pointing the telescope at\n",
      "  new parts of the sky.\n",
      "- get more features for the existing data.  This involves observing\n",
      "  the same objects over again, perhaps with telescopes which are\n",
      "  sensitive to different regions of the electromagnetic spectrum.\n",
      "\n",
      "We'll use learning curves to determine this.\n",
      "\n",
      "We're now going to plot the training error and cross-validation error\n",
      "as a function of the number of training samples.  Remember to\n",
      "**Make sure** that when computing the training error for each number of\n",
      "samples, you use the same samples that the model was trained on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we'll explore 20 training set sizes\n",
      "# sample them evenly between 50 and the number of training samples\n",
      "n_samples_array = np.linspace(50, Ntrain, 20).astype(int)\n",
      "train_error_2 = np.zeros(n_samples_array.shape)\n",
      "cv_error_2 = np.zeros(n_samples_array.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  Here you will fill-in the train_error_2 array and the cv_error_2 array\n",
      "#  with values that correspond to the number of samples.\n",
      "#  Make sure that when computing the training error for each number of\n",
      "#  samples, you use the same samples that the model was trained on.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the notebook is within the tutorial directory structure,\n",
      "the following command will load the solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy soln/02-02.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.plot(n_samples_array, cv_error_2, label='cross-val error')\n",
      "pl.plot(n_samples_array, train_error_2, label='training error')\n",
      "\n",
      "pl.legend(loc=0)\n",
      "pl.xlabel('number of samples')\n",
      "pl.ylabel('error')\n",
      "pl.title('max_depth = %i' % max_depth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on this, we can guess that the best possible rms error for the\n",
      "Decision Tree Regressor with the given value of ``max_depth`` is around 0.2,\n",
      "and that using more samples will not significantly improve the results\n",
      "(the two lines will simply approach each other asymtotically).\n",
      "\n",
      "Note, though, that if more samples were added, a higher value of\n",
      "``max_depth`` might be a better choice.  These lines are really\n",
      "two-dimensional slices of surfaces in three dimensions:\n",
      "\n",
      "     (number of samples) x (max_depth) x (rms error)\n",
      "\n",
      "You can re-run this analysis using more points by commenting-out the line\n",
      "\n",
      "    #data = data[:50000]\n",
      "\n",
      "which is found near the beginning of the notebook,\n",
      "and choosing \"Run All\" from the ipython menu.  Note that this\n",
      "might take a very long time to execute, depending on how fast your computer\n",
      "is.  How does this change the results?  Is it as you would expect?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 3: A different metric"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RMS error is not always the best metric to use.  In the case\n",
      "of photometric redshifts, the *catastrophic errors* mentioned\n",
      "above can be much more damaging to scientific results.  For\n",
      "that reason, we might want to use a different metric when\n",
      "exploring the above results.\n",
      "\n",
      "We'll define a convenience function to compute the outlier fraction,\n",
      "and use this as a measure of how well the model performs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_outlier_fraction(y_pred, y_true, cutoff=0.2):\n",
      "    return np.sum((abs(y_pred - y_true) > cutoff)) * 1. / len(y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, your task is to re-implement what's above using the outlier fraction\n",
      "rather than the rms error as a cost function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat the two previous plots using compute_outlier_fraction() rather\n",
      "# than compute_rms_error().\n",
      "# feel free to make use of copying and pasting!\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the notebook is within the tutorial directory structure,\n",
      "the following commands will load the solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy soln/02-03a.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy soln/02-03b.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compared to the rms error, does the outlier fraction prefer more or less\n",
      "complicated models?  Would you recommend more data?  A more complicated model?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bonus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One way to improve on Decision Trees is to use *Ensemble Methods*.\n",
      "Ensemble methods (also known as *boosting* and *bagging*) use ensembles\n",
      "of randomized estimators which can prevent over-fitting and lead to very\n",
      "powerful learning algorithms.\n",
      "\n",
      "It is interesting to see how small an RMS you can attain on the photometric\n",
      "redshift problem using a more sophisticated method.  Try one of the following:\n",
      "\n",
      "- ``sklearn.ensemble.RandomForestRegressor``\n",
      "- ``sklearn.ensemble.GradientBoostingRegressor``\n",
      "- ``sklearn.ensemble.ExtraTreesRegressor``\n",
      "\n",
      "You can read more about each of these methods in the scikit-learn documentation:\n",
      "\n",
      "- http://scikit-learn.org/stable/modules/ensemble.html\n",
      "\n",
      "Each method has hyperparameters which need to be determined using cross-validation\n",
      "steps like those above.  Can you get the rms error for the test set\n",
      "down below 0.1?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}

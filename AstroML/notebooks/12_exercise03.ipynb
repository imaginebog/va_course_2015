{
 "metadata": {
  "name": "12_exercise03"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise 3: Dimensionality Reduction of Spectra"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this tutorial, we explore manifold learning techniques to visualize 4000\n",
      "SDSS spectral data.  This is a much more exploratory exercise than the previous\n",
      "two.  The goal is to determine how to best visualize this high-dimensional\n",
      "space.  You will implement PCA, LLE, Modified LLE, and Isomap, for various\n",
      "data normalizations.  The goal is to find the best visualization of the\n",
      "data, where \"best\" in this case is a qualitative measure of how well the\n",
      "different classes of points are separated in the projected space.\n",
      "\n",
      "Because we're going to be plotting things below, we'll first make sure we're in\n",
      "ipython's pylab mode:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as pl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial assumes the notebook is within the tutorial directory structure,\n",
      "and that the fetch_data.py script has been run to download the data locally.\n",
      "If the data is in a different location, you can change the DATA_HOME variable below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "DATA_HOME = os.path.abspath('../data/sdss_spectra')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "data = np.load(os.path.join(DATA_HOME, 'spec4000_corrected.npz'))\n",
      "\n",
      "X = data['X']\n",
      "y = data['y']\n",
      "labels = data['labels']\n",
      "\n",
      "# shuffle the data\n",
      "i = np.arange(y.shape[0], dtype=int)\n",
      "np.random.shuffle(i)\n",
      "X = X[i]\n",
      "y = y[i]\n",
      "\n",
      "print X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we see that the dataset consists of 4000 points in 1000 dimensions.\n",
      "Let's plot a few of the spectra so we can see what they look like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_spectral_types(data):\n",
      "    X = data['X']\n",
      "    y = data['y']\n",
      "    wavelengths = data['wavelengths']\n",
      "    labels = data['labels']\n",
      "\n",
      "    for i_class in (2, 3, 4, 5, 6):\n",
      "        i = np.where(y == i_class)[0][0]\n",
      "        l = pl.plot(wavelengths, X[i] + 20 * i_class)\n",
      "        c = l[0].get_color()\n",
      "        pl.text(6800, 2 + 20 * i_class, labels[i_class], color=c)\n",
      "   \n",
      "    pl.subplots_adjust(hspace=0)\n",
      "    pl.xlabel('wavelength (Angstroms)')\n",
      "    pl.ylabel('flux + offset')\n",
      "    pl.title('Sample of Spectra')\n",
      "    \n",
      "plot_spectral_types(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see what the 1000 dimensions mean: for each object, there is a measurement\n",
      "in each of 1000 wavelength bins.  In other words, these objects exist in a\n",
      "1000 dimensional parameter space.  If we could draw a graph in 1000 dimensions,\n",
      "each spectrum could be represented by a single point in this 1000 dimensional space.\n",
      "\n",
      "Unfortunately, it is very difficult for us to visualize even four or five dimensions,\n",
      "let alone 1000.  This is why it is often useful to think about finding an optimal\n",
      "lower-dimensional projection of the dataset, where optimal is defined in some\n",
      "quantitative way.\n",
      "\n",
      "In this exercise we will be visualizing several three-dimensional projections\n",
      "of high dimensional data.  To streamline this, we'll first define a function\n",
      "which lets us scatter-plot three dimensions of data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def three_component_plot(c1, c2, c3, color, labels,\n",
      "                         trim_outliers=True, sigma_cutoff=3):\n",
      "    if trim_outliers:\n",
      "        mask = np.zeros(c1.shape, dtype=bool)\n",
      "        for c in [c1, c2, c3]:\n",
      "            c = np.asarray(c)\n",
      "            mu = np.mean(c)\n",
      "            std = np.std(c)\n",
      "            mask |= (c > mu + sigma_cutoff * std)\n",
      "            mask |= (c < mu - sigma_cutoff * std)\n",
      "        \n",
      "        print \"removing %i outliers\" % mask.sum()\n",
      "        \n",
      "        c1 = c1[~mask]\n",
      "        c2 = c2[~mask]\n",
      "        c3 = c3[~mask]\n",
      "        color = color[~mask]\n",
      "    \n",
      "    fig = pl.figure(figsize=(8,8))\n",
      "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
      "    \n",
      "    kwargs = dict(s=8, lw=0, c=color, vmin=2, vmax=6)\n",
      "    ax1 = fig.add_subplot(221)\n",
      "    pl.scatter(c1, c2, **kwargs)\n",
      "    ax1.set_ylabel('component 2')\n",
      "\n",
      "    ax2 = fig.add_subplot(223, sharex=ax1)\n",
      "    pl.scatter(c1, c3, **kwargs)\n",
      "    ax2.set_xlabel('component 1')\n",
      "    ax2.set_ylabel('component 3')\n",
      "\n",
      "    ax3 = fig.add_subplot(224, sharey=ax2)\n",
      "    pl.scatter(c2, c3, **kwargs)\n",
      "    ax3.set_xlabel('component 2')\n",
      "\n",
      "    for ax in (ax1, ax2, ax3):\n",
      "        ax.xaxis.set_major_formatter(pl.NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(pl.NullFormatter())\n",
      "        \n",
      "        ax.axis('tight')\n",
      "\n",
      "    color_format = pl.FuncFormatter(lambda i, *args: labels[i])\n",
      "    pl.colorbar(ticks = range(2, 7), format=color_format,\n",
      "                cax = pl.axes((0.52, 0.51, 0.02, 0.39)))\n",
      "    pl.clim(1.5, 6.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try this out right now: we'll use three randomly-drawn\n",
      "dimensions for the points within the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "three_component_plot(X[:, 100], X[:, 200], X[:, 300], y, labels, trim_outliers=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that there are some strong correlations between the different\n",
      "dimensions.  In particular, component 2 and 3 seem to measure nearly\n",
      "the same information.\n",
      "\n",
      "As we saw in the earlier exercise, one possible projection to use is\n",
      "based on Principal Component Analysis.  This looks for the linear\n",
      "combination of data attributes which show the largest variance, and\n",
      "thus are in some sense the most \"important\" combination of features.\n",
      "\n",
      "We'll want to experiment with different numbers of samples, and\n",
      "different normalizations, so we'll create a quick convenience\n",
      "function to streamline this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "\n",
      "def preprocess(data, shuffle=True, n_samples=1000, normalization=None):\n",
      "    \"\"\"Preprocess the data\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    shuffle: True or False\n",
      "    n_samples: integer between 1 and 4000\n",
      "    normalization: None or 'L1' or 'L2'\n",
      "    \"\"\"\n",
      "    X = data['X']\n",
      "    y = data['y']\n",
      "    \n",
      "    # shuffle the data\n",
      "    if shuffle:\n",
      "        i = np.arange(y.shape[0], dtype=int)\n",
      "        np.random.shuffle(i)\n",
      "        X = X[i]\n",
      "        y = y[i]\n",
      "    \n",
      "    # truncate the data\n",
      "    X = X[:n_samples]\n",
      "    y = y[:n_samples]\n",
      "    \n",
      "    # normalize the data\n",
      "    if not normalization:\n",
      "        pass\n",
      "    elif normalization.lower() == 'l2':\n",
      "        X = preprocessing.normalize(X, 'l2')\n",
      "    elif normalization.lower() == 'l1':\n",
      "        X = preprocessing.normalize(X, 'l1')\n",
      "    else:\n",
      "        raise ValueError(\"Unrecognized normalization: '%s'\" % normalization)\n",
      "        \n",
      "    return X, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Principal Component Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll perform the randomized PCA projection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Principal Component Analysis\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "\n",
      "X, y = preprocess(data, shuffle=False, n_samples=1000, normalization='L2')\n",
      "rpca = RandomizedPCA(n_components=3, random_state=0)\n",
      "X_proj = rpca.fit_transform(X)\n",
      "\n",
      "three_component_plot(X_proj[:, 0], X_proj[:, 1], X_proj[:, 2], y, labels, trim_outliers=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The PCA projection allows us to visualize the data in a meaningful way.\n",
      "In particular, it shows that the absorption galaxies (blue points) do\n",
      "occupy a different region of parameter space than the emission galaxies\n",
      "(green points), and that quasars (orange and red points) are relatively\n",
      "rare in the dataset.\n",
      "This sort of projection has been successfully used, for example,\n",
      "as a projection method to understand the relationship between galaxies\n",
      "and quasars, and also a step toward automated classification based on\n",
      "spectra (see Yip *et al.* 2004).\n",
      "\n",
      "But the weakness of PCA is that it is a *linear* projection, and features\n",
      "that distinguish quasars (red and orange points) from emission galaxies\n",
      "(green points) are non-linear: for this reason, nonlinear projections can\n",
      "be a better choice (see Vanderplas *et al.* 2009).  In this exercise we\n",
      "will explore several of these projection methods.  For a description of\n",
      "the available methods, see http://scikit-learn.org/stable/modules/manifold.html."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Locally Linear Embedding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First let's try *Locally Linear Embedding* (available in ``sklearn.manifold.LLE``)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here create an object from sklearn.manifold.LLE with n_components=3 and method='standard'\n",
      "# you will have to select n_neighbors.  15 is a good first guess.\n",
      "# initialize the object, fit on the dataset X, and compute the projection X_proj\n",
      "# The syntax is similar to RandomizedPCA, above\n",
      "\n",
      "# on older versions of scikit-learn, out_dim is used rather than n_components\n",
      "\n",
      "#X, y = preprocess(...\n",
      "\n",
      "# perform LLE fit here\n",
      "\n",
      "#three_component_plot(...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the notebook is within the tutorial directory structure,\n",
      "the following command will load the solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy soln/03-01.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What do you notice about this projection?  What are the effects of normalizing\n",
      "or not normalizing?  How does the number of neighbors affect the results?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modified Locally Linear Embedding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll try out modified LLE (MLLE).  MLLE essentially uses multiple\n",
      "coefficients in each neighborhood to better preserve the local geometry\n",
      "of the data in the projection.  You can used modified LLE by copying the\n",
      "above LLE code, but with the keyword `method = 'modified'`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train and plot the MLLE projection of the data here\n",
      "# use the LLE code from above as a template.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the notebook is within the tutorial directory structure,\n",
      "the following command will load the solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy soln/03-02.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How does this projection compare to that of standard LLE?  Does this\n",
      "projection lead to a more intuitive representation of the relationship\n",
      "between the points?  Experiment with a few choices of n_neighbors, and\n",
      "normalization vs. no normalization.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Isomap"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final technique we'll explore is *Isomap*, short for \"Isometric Mapping\".\n",
      "It's an approach to the same problem that is based on graph theory.  The\n",
      "scikit-learn implementation is in ``sklearn.manifold.Isomap``.  As above,\n",
      "you should ``preprocess`` the data, compute the Isomap projection, and then\n",
      "plot the results with our ``three_component_plot`` function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute and plot the isomap projection here:\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the notebook is within the tutorial directory structure,\n",
      "the following command will load the solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy soln/03-03.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#03-03.py\n",
      "X, y = preprocess(data, shuffle=False, n_samples=1000, normalization=None)\n",
      "\n",
      "from sklearn.manifold import Isomap\n",
      "iso = Isomap(n_neighbors=15, n_components=3)\n",
      "X_proj = iso.fit_transform(X)\n",
      "\n",
      "three_component_plot(X_proj[:, 0], X_proj[:, 1], X_proj[:, 2], y, labels, trim_outliers=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, explore the different options for the projection.  How does the\n",
      "normalization, the number of neighbors, etc. affect the resulting projection?\n",
      "Between PCA, LLE, MLLE, and Isomap, which method leads to the most useful\n",
      "visualization of the data (i.e. which could be used to construct and intuitive\n",
      "model for a simple classification of the different objects?)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This has been a fairly qualitative exploration of several projection\n",
      "and visualization methods available in scikit-learn.  These have been\n",
      "explored in more detail in several papers, a selection of which can be\n",
      "found below:\n",
      "\n",
      "- [Yip *et al.* 2004a](http://adsabs.harvard.edu/abs/2004AJ....128.2603Y)\n",
      "  explores PCA of SDSS quasar spectra.\n",
      "- [Yip *et al.* 2004b](http://adsabs.harvard.edu/abs/2004AJ....128..585Y)\n",
      "  explores PCA of SDSS galaxy spectra.\n",
      "- [Vanderplas *et al.* 2009](http://adsabs.harvard.edu/abs/2009AJ....138.1365V)\n",
      "  explores LLE of SDSS galaxy and quasar spectra\n",
      "- [Daniel *et al.* 2011](http://adsabs.harvard.edu/abs/2011AJ....142..203D)\n",
      "  explores LLE of SDSS stellar spectra\n",
      "- [Gal *et al.* 2012a](http://adsabs.harvard.edu/abs/2012AJ....143..123M)\n",
      "  explores LLE as a preprocessing step for Kepler light curves (i.e. time-domain\n",
      "  data)\n",
      "- [Gal *et al.* 2012b](http://adsabs.harvard.edu/abs/2012ApJS..200...14M)\n",
      "  explores LLE as a preprocessing step for classification of RAVE spectra.\n",
      "\n",
      "There are still many unanswered questions in the use of manifold learning methods,\n",
      "both in general and in Astronomical applications."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}

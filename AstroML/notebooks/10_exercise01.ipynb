{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Star/Quasar Classification with GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial, we used a Gaussian Naive Bayes Classifier to separate Quasars\n",
    "And Stars.  In this exercise, we will extend this classification scheme\n",
    "using Gaussian Mixture Models.\n",
    "\n",
    "The Gaussian Naive Bayes method works by computing a Gaussian approximation\n",
    "to the distribution of each training class, and using these distributions\n",
    "to predict the most probable label for any new point.\n",
    "\n",
    "Here we're going to extend this, and rather than fitting a *single* Gaussian\n",
    "to each label distribution, we're going to fit a *mixture* of Gaussians, which\n",
    "should better approximate the distribution of each class of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a brief backround on what Gaussian Naive Bayes is.\n",
    "Suppose you have a dataset consisting of two clusters of points.\n",
    "We'll generate them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "np.random.seed(0)\n",
    "X = np.vstack([np.random.normal(0, 1, size=(100, 2)),\n",
    "               np.random.normal(3, 1, size=(100, 2))])\n",
    "y = np.zeros(200)\n",
    "y[100:] = 1\n",
    "\n",
    "pl.scatter(X[:, 0], X[:, 1], c=y, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the source code of ``sklearn.naive_bayes.GaussianNB``,\n",
    "you'll see that internally it finds a best-fit Gaussian for each\n",
    "distribution, and uses these as a smooth description of each distribution.\n",
    "We can use the internals of ``GaussianNB`` to visualize the distributions\n",
    "it uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X, y)\n",
    "\n",
    "print(gnb.theta_) # centers of the distributions\n",
    "print(gnb.sigma_) # widths of the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a grid on which to evaluate the distributions\n",
    "grid = np.linspace(-3, 6, 100)\n",
    "xgrid, ygrid = np.meshgrid(grid, grid)\n",
    "Xgrid = np.vstack([xgrid.ravel(), ygrid.ravel()]).T\n",
    "\n",
    "# now evaluate and plot the probability grid\n",
    "prob_grid = np.exp(gnb._joint_log_likelihood(Xgrid))\n",
    "for i, c in enumerate(['blue', 'red']):\n",
    "    pl.contour(xgrid, ygrid, prob_grid[:, i].reshape((100, 100)), 3, colors=c)\n",
    "\n",
    "# plot the points as above\n",
    "pl.scatter(X[:, 0], X[:, 1], c=y, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a new item is to be classified, its probability is evaluated\n",
    "for each distribution, and the distribution with the highest\n",
    "probability wins.  We can see now why this is called \"naive\".  What\n",
    "if the distribution is not well-fit by an uncorrelated Gaussian?\n",
    "\n",
    "In the following, we'll develop a classifier which solves this issue\n",
    "by fitting a *sum* of gaussians to each distribution.  This should\n",
    "lead to improved classifications.  For our data, we'll use photometric\n",
    "observations of stars and quasars from the Sloan Digital Sky Survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes the notebook is within the tutorial directory\n",
    "structure, and that the ``fetch_data.py`` script has been run\n",
    "to download the data locally.\n",
    "If the data is in a different location, you can change the\n",
    "``DATA_HOME`` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_HOME = os.path.abspath('../data/sdss_colors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data = np.load(os.path.join(DATA_HOME,\n",
    "                                  'sdssdr6_colors_class_train.npy'))\n",
    "test_data = np.load(os.path.join(DATA_HOME,\n",
    "                                 'sdssdr6_colors_class.200000.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the number of training points: using all points leads to a very\n",
    "# long running time.  We'll start with 10000 training points.  This\n",
    "# can be increased if desired.\n",
    "Ntrain = 10000\n",
    "#Ntrain = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split training data into training and cross-validation sets\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(train_data)\n",
    "train_data = train_data[:Ntrain]\n",
    "\n",
    "N_crossval = Ntrain / 5\n",
    "train_data = train_data[:-N_crossval]\n",
    "crossval_data = train_data[-N_crossval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct training data\n",
    "X_train = np.zeros((train_data.size, 4), dtype=float)\n",
    "X_train[:, 0] = train_data['u-g']\n",
    "X_train[:, 1] = train_data['g-r']\n",
    "X_train[:, 2] = train_data['r-i']\n",
    "X_train[:, 3] = train_data['i-z']\n",
    "y_train = (train_data['redshift'] > 0).astype(int)\n",
    "Ntrain = len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct cross-validation data\n",
    "X_crossval = np.zeros((crossval_data.size, 4), dtype=float)\n",
    "X_crossval[:, 0] = crossval_data['u-g']\n",
    "X_crossval[:, 1] = crossval_data['g-r']\n",
    "X_crossval[:, 2] = crossval_data['r-i']\n",
    "X_crossval[:, 3] = crossval_data['i-z']\n",
    "y_crossval = (crossval_data['redshift'] > 0).astype(int)\n",
    "Ncrossval = len(y_crossval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for good measure, let's plot the first two dimensions of the data\n",
    "to see a bit of what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=4, linewidths=0)\n",
    "pl.xlim(-2, 5)\n",
    "pl.ylim(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have training distributions which are fairly well-separated.\n",
    "Note, though, that these distributions are not well-approximated\n",
    "by a single Gaussian!  Still, Gaussian Naive Bayes can be a\n",
    "useful classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Recreating Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes is a very fast estimator, and predicted labels can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_gnb = gnb.predict(X_crossval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use Gaussian Mixture Models to duplicate our Gaussian\n",
    "Naive Bayes results from earlier.  You'll create two ``sklearn.gmm.GMM()``\n",
    "classifier instances, named ``clf_0`` and ``clf_1``.  Each should be\n",
    "initialized with a single component, and diagonal covariance.\n",
    "(hint: look at the doc string for ``sklearn.gmm.GMM`` to see how to set\n",
    "this up).  The results should be compared to Gaussian Naive Bayes\n",
    "to check if they're correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#   Objects to create:\n",
    "#    - clf_0 : trained on the portion of the training data with y == 0\n",
    "#    - clf_1 : trained on the portion of the training data with y == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the notebook is within the tutorial directory structure,\n",
    "the following command will load the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load soln/01-01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 01-01.py\n",
    "clf_0 = gmm.GMM(1, 'diag')\n",
    "i0 = (y_train == 0)\n",
    "clf_0.fit(X_train[i0])\n",
    "\n",
    "clf_1 = gmm.GMM(1, 'diag')\n",
    "i1 = (y_train == 1)\n",
    "clf_1.fit(X_train[i1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must construct the prior.  The prior is the fraction of training\n",
    "points of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variables to compute:\n",
    "#  - prior0 : fraction of training points with y == 0\n",
    "#  - prior1 : fraction of training points with y == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the notebook is within the tutorial directory structure,\n",
    "the following command will load the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load soln/01-02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the prior and the classifiation to compute the log-likelihoods\n",
    "of the cross-validation points.  The log likelihood for a test point ``x``\n",
    "is given by\n",
    "   \n",
    "     logL(x) = clf.score(x) + log(prior)\n",
    "\n",
    "You can use the function ``np.log()`` to compute the logarithm of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  variables to compute:\n",
    "#    logL : array, shape = (2, Ncrossval)\n",
    "#            logL[0] is the log-likelihood for y == 0\n",
    "#            logL[1] is the log-likelihood for y == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the notebook is within the tutorial directory structure,\n",
    "the following command will load the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load soln/01-03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once ``logL`` is computed, the predicted value for each sample\n",
    "is the index with the largest log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(logL, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to GNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare our predicted labels to ``y_gnb``, the labels we\n",
    "printed above.  We'll use the built-in classification\n",
    "report function in sklearn.metrics.  This computes the precision,\n",
    "recall, and f1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"One-component Gaussian Mixture:\"\n",
    "print metrics.classification_report(y_crossval, y_pred,\n",
    "                                    target_names=['stars', 'QSOs'])\n",
    "\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"Gaussian Naive Bayes:\"\n",
    "print metrics.classification_report(y_crossval, y_gnb,\n",
    "                                    target_names=['stars', 'QSOs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the results of these two should be identical.  In reality, the\n",
    "two algorithms approach the fits differently, leading to slightly different\n",
    "results.  The precision, recall, and F1-score should match to within ~0.01.\n",
    "If this is the case, then we can go on and experiment with a more complicated\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take some time to experiment with the *hyperparameters* of our\n",
    "GMM Bayesian classifier.  These include the number of components for each\n",
    "model and the covariance type for each model (i.e. parameters that are\n",
    "decided prior to the fitting of the model on training data.\n",
    "\n",
    "Note that for a large number of components, the fit can take a long\n",
    "time, and will be dependent on the starting position.  Use the\n",
    "documentation string of GMM to determine the options for covariance.\n",
    "\n",
    "Note that there are tools within scikit-learn to perform hyperparameter\n",
    "estimation, in the module ``sklearn.grid_search``.\n",
    "Here we will be doing it by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this exercise is to re-implement the GMM estimator above\n",
    "in a single function which allows the number of clusters and covariance type\n",
    "to be specified.  To follow the scikit-learn syntax, this should be a class\n",
    "with methods like ``fit()``, ``predict()``, ``predict_proba()``, etc.\n",
    "This would be an interesting project (and could even be a useful contribution\n",
    "to scikit-learn!).  For now, we'll take a shortcut and just define a\n",
    "stand-alone function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finish this function.  For n_components=1 and\n",
    "# covariance_type='diag', it should give results\n",
    "# identical to what we saw above.\n",
    "\n",
    "# the function should return the predicted\n",
    "# labels y_pred.\n",
    "def GMMBayes(X_test, n_components, covariance_type):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the notebook is within the tutorial directory structure,\n",
    "the following command will load the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load soln/01-04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply this to our training data, and take a look at the F1-score\n",
    "as a function of the hyperparameters.  Here is an example of computing the\n",
    "F1-score for a particular choice of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "covariance_type = 'full'\n",
    "\n",
    "y_pred = GMMBayes(X_train, n_components, 'full')\n",
    "f1 = metrics.f1_score(y_train, y_pred)\n",
    "\n",
    "print f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the number of components and the covariance type.\n",
    "To see a description of the various ``covariance_type`` options,\n",
    "you can type\n",
    "\n",
    "     gmm.GMM?\n",
    "\n",
    "in a code cell to see the documentation.  You might also wish to\n",
    "loop over several values of the hyperparameters and plot the\n",
    "learning curves for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have settled on a choice of hyperparameters, it's time to\n",
    "evaluate the test data using this model.  First we'll construct the\n",
    "test data as we did the training and cross-validation data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = np.zeros((test_data.size, 4), dtype=float)\n",
    "X_test[:, 0] = test_data['u-g']\n",
    "X_test[:, 1] = test_data['g-r']\n",
    "X_test[:, 2] = test_data['r-i']\n",
    "X_test[:, 3] = test_data['i-z']\n",
    "y_pred_literature = (test_data['label'] == 0).astype(int)\n",
    "Ntest = len(y_pred_literature)\n",
    "\n",
    "print Ntest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follow the procedure above, and for the test data predict the\n",
    "labels using the Gaussian Naive Bayes estimator, as well as our\n",
    "Gaussian Mixture Bayes estimator.  For simplicity, you may wish to\n",
    "use the Gaussian Mixture estimator to evaluate the Naive Bayes\n",
    "result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variables to compute:\n",
    "#   y_pred_gmm : predicted labels for X_test from GMM Bayes model\n",
    "#   y_pred_gnb : predicted labels for X_test with Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the notebook is within the tutorial directory structure,\n",
    "the following command will load the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load soln/01-05.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"------------------------------------------------------------------\"\n",
    "print \"Comparison of current results with published results (Naive Bayes)\"\n",
    "print metrics.classification_report(y_pred_literature, y_pred_gnb,\n",
    "                                    target_names=['stars', 'QSOs'])\n",
    "\n",
    "print \"------------------------------------------------------------------\"\n",
    "print \"Comparison of current results with published results (GMM Bayes)\"\n",
    "print metrics.classification_report(y_pred_literature, y_pred_gmm,\n",
    "                                    target_names=['stars', 'QSOs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're on the right track, you should find that the GMM-based\n",
    "model has led to a classifier more consistent with the results\n",
    "from the literature.\n",
    "This is an example where the Naive Bayes model was too simple: it\n",
    "under-fit the data, and thus increasing the model complexity has\n",
    "led to a better classification algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

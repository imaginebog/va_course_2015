{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of Iris Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the task of gathering samples into groups of similar\n",
    "samples according to some predefined similarity or dissimilarity\n",
    "measure (such as the Euclidean distance).\n",
    "\n",
    "Let's re-use the results of the 2D PCA of the iris dataset in order to\n",
    "explore clustering.  First we need to repeat some of the code from the\n",
    "previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure ipython inline mode is activated\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all of this is taken from the notebook '03_iris_dimensionality.ipynb' \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True).fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "def plot_2D(data, target, target_names):\n",
    "    colors = cycle('rgbcmykw')\n",
    "    target_ids = range(len(target_names))\n",
    "    pl.figure()\n",
    "    for i, c, label in zip(target_ids, colors, target_names):\n",
    "        pl.scatter(data[target == i, 0], data[target == i, 1],\n",
    "                   c=c, label=label)\n",
    "    pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use one of the simplest clustering algorithms, K-means.\n",
    "This is an iterative algorithm which searches for three cluster\n",
    "centers such that the distance from each point to its cluster is\n",
    "minimizied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=rng).fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.round(kmeans.cluster_centers_, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means algorithm has been used to infer cluster labels for the\n",
    "points.  Let's call the ``plot_2D`` function again, but color the points\n",
    "based on the cluster labels rather than the iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_2D(X_pca, kmeans.labels_, [\"c0\", \"c1\", \"c2\"])\n",
    "\n",
    "plot_2D(X_pca, iris.target, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the K-Means cluster search again, but this time learn the\n",
    "clusters using the full data matrix ``X``, rather than the projected\n",
    "matrix ``X_pca``.  Does this change the results?  Do these labels\n",
    "look closer to the true labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Means algorithm depends on the random initial placements of the first centroids. In the example you'll always obtain the same placement because the random state is fixed with the command rng = RandomState(42).\n",
    "Repeat a few times the  K-Means cluster search with a true random state and compare the results. Share your thoughts about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
